:toc:
:toc-placement!:
:toclevels: 4

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= How Emendate processes date strings

WARNING:: As of 2023-03-31, this is out of date. The overall flow of processing remains the same, but the way it is accomplished, and how you can access intermediate step data has all changed.

toc::[]

== Basic conceptual overview

The overarching idea is to run the string through a succession of transformational processes, each of which simplifies/standardizes some of the complexity.

The result of each of these stages is a `SegmentSet` (or subclass thereof), which is an ordered list of ``Segment``s (or subclasses thereof).

A `Token` is the simplest type of `Segment`, being derived directly from the original string by the `Lexer`.

Subsequent processes distill the ``Token``s (4 digit number, hyphen, 1-or-2 digit number) into ``DatePart``s (year, month), which are then distilled into ``DateType``s (year-month level date, year-month-day level date, etc.).

Each more complex type of `Segment` has its own logic and functionality. For example a `DatePart` with `type` = `day` can carry with it `certainty` = `[:uncertain, :approximate]`. Every `DateType` can output an `earliest` and `latest` value and say whether it represents an inclusive range.

Once we are down to just ``DateType``s and separators, a final `ParsedDate` object can be derived from them.

The `ParsedDate` object is modeled on the response returned by Timetwister.

== Processing stages

Calling `Emendate.process('date string')` invokes an instance of the `ProcessingManager` class, which handles the workflow of processing the input.

Each input value goes through a number of processing stages. If processing fails at one stage, that causes the overall processing to fail.

The order and basic function of each stage is outlined below, with examples.

NOTE: To get a quick look at a date pattern, we can use the `types` method on anything returning a `SegmentSet`. This lists the token/segment type of each part.

=== `orig_tokens`

*Produced by processing step: `:lex`*

Original date string is normalized, then run through `Lexer`, resulting in a `TokenSet`.

----
ex1 = Emendate.process('[Jan. 1st - February 9, 2013]')
ex1.orig_tokens.types
 => [:square_bracket_open,
 :month_abbr_alpha,
 :single_dot,
 :space,
 :number1or2,
 :ordinal_indicator,
 :space,
 :hyphen,
 :space,
 :month_alpha,
 :space,
 :number1or2,
 :comma,
 :space,
 :number4,
 :square_bracket_close]
----

At this stage, we are trying to just capture the very basic patterns of the string with very minor abstraction. For instance, we are able to recognize the English months and standard month abbreviations and convert those to tokens with the types `:month_alpha` and `:month_abbr_alpha`.

NOTE: We can also use the `date_part_types` method on anything returning a `SegmentSet`. This will list the token/segment type of only parts that are `DatePart` segments or ``Token``s with a type known to indicate a date part.

[source, ruby]
----
ex1.orig_tokens.date_part_types
 => [:month_abbr_alpha, :number1or2, :month_alpha, :number1or2, :number4]
----

=== `tagged_untokenizable`

*Produced by processing step: `:tag_untokenizable`*

If no `orig_tokens` are of type `:unknown`, everything is passed through with no change and processing continues:

[source,ruby]
----
ex1.tagged_untokenizable.types
 => [:square_bracket_open,
 :month_abbr_alpha,
 :single_dot,
 :space,
 :number1or2,
 :ordinal_indicator,
 :space,
 :hyphen,
 :space,
 :month_alpha,
 :space,
 :number1or2,
 :comma,
 :space,
 :number4,
 :square_bracket_close]

ex1.tagged_untokenizable.date_part_types
 => [:month_abbr_alpha, :number1or2, :month_alpha, :number1or2, :number4]
----

We'll use another example to look at what happens when one or more `orig_tokens` is tagged as type `:unknown`:

[source, ruby]
----
ex2 = Emendate.process('about 1970')
 => #<Emendate::ProcessingManager:1620
  @state=done,
  token_type_pattern: [:untokenizable_date_type]> <1>

ex2.orig_tokens.types
 => [:unknown, :space, :number4] <2>

ex2.tagged_untokenizable.types
 => [:untokenizable_date_type] <3>

ex2.warnings
 => ["Untokenizable sequences: about"] <4>

ex2.tagged_unprocessable.types
  NoMethodError: undefined method `types' for nil:NilClass <5>

ex2.result
=> #<Emendate::Result:0x00007fdfd10bb658
 @dates=
  [#<Emendate::ParsedDate:0x00007fdfd10bb900
    @certainty=[],
    @date_end=nil,
    @date_end_full=nil,
    @date_start=nil,
    @date_start_full=nil,
    @inclusive_range=nil,
    @index_dates=[],
    @original_string="about 1970">],
 @errors=[],
 @original_string="about 1970",
 @warnings=["Untokenizable sequences: about"]> <6>
----
<1> The displayed representation of the `ProcessingManager` always shows the current/final token type pattern, so this tips us off to what happens when there are tokens with type `:unknown`
<2> Here we see that the lexing step could not tokenize `about` so it's token has type = `:unknown`
<3> Because we already know we aren't going to be able strings with unrecognized/unidentifiable patterns, we are going to call the whole string an `:untokenizable_date_type` and stop processing.
<4> Any untokenizeable sequences are listed in a warning on the `ProcessingManager`, to be used however you need to use such info
<5> Because processing was stopped, the next step didn't run, so there are no more `types` to look at
<6> The warning also makes it into the `Emendate::Result`, which also contains a single `ParsedDate` object with no information other than the original string value.

The rationale for this treatement of untokenizable strings is:

* If you are doing something strict with date parsing, the useful parsed info is all nil
* If you are doing something less strict, you can easily fall back to using original string

Either way, no error is raised, because we expect there are *always* going to be weird, unrecognized strings in date data sets. It is not an exceptional, error-worthy situation.

=== `tagged_unprocessable`

*Produced by processing step: `:tag_unprocessable`*

This step handles known patterns that we cannot handle and do not expect to implement any time in the immediate future, so they don't throw errors. We know what is going to happen with them, so we can treat them in a consistent, expected way.

The known unprocessable patterns are hand-coded regular expressions in a constant that you can see in the console by running:

[source, ruby]
----
Emendate::UnprocessableTagger::Patterns
----

Again, this doesn't do anything to a date string we can actually deal with, so we will skip looking at `ex1` here again.

[source, ruby]
----
ex3 = Emendate.process('XXXX-10-XX')
 => #<Emendate::ProcessingManager:1640
  @state=done,
  token_type_pattern: [:unprocessable_date_type]>

ex3.tagged_unprocessable.types
 => [:unprocessable_date_type]

ex3.tagged_known_unknown.types
NoMethodError: undefined method `types' for nil:NilClass

ex3.result
=> #<Emendate::Result:0x00007f90ec1a3a40
 @dates=
  [#<Emendate::ParsedDate:0x00007f90ec1a3db0
    @certainty=[],
    @date_end=nil,
    @date_end_full=nil,
    @date_start=nil,
    @date_start_full=nil,
    @inclusive_range=nil,
    @index_dates=[],
    @original_string="XXXX-10-XX">],
 @errors=[],
 @original_string="XXXX-10-XX",
 @warnings=["Unprocessable string"]>
----

This behavior pattern is exactly the same as for untokenizable segments, but it is explicit about the reason the date is not processed/parsed further.

=== `tagged_known_unknown`

*Produced by processing step: `:tag_known_unknown`*

This step handles patterns that express the fact of an unknown date, such as `n.d.` or `unknown`.

This is going to look very familiar...

[source, ruby]
----
ex4 = Emendate.process('n.d.')
 => #<Emendate::ProcessingManager:1540
  @state=done,
  token_type_pattern: [:knownunknown_date_type]>

ex4.orig_tokens.types
 => [:unknown_date]

ex4.tagged_known_unknown.types
 => [:knownunknown_date_type]

ex4.collapsed_tokens.types
NoMethodError: undefined method `types' for nil:NilClass

ex4.result
=> #<Emendate::Result:0x00007fa4081d4418
 @dates=[#<Emendate::ParsedDate:0x00007fa4081d4710 @certainty=[], @date_end=nil, @date_end_full=nil, @date_start=nil, @date_start_full=nil, @inclusive_range=nil, @index_dates=[], @original_string="n.d.">],
 @errors=[],
 @original_string="n.d.",
 @warnings=[]>
----

The only difference here is there is no warning in the result.

=== `collapsed_tokens`

*Produced by processing step: `:collapse_tokens`*

This step simplifies the token type pattern by collapsing non-meaningful tokens. We used the term "collapse" instead of "delete" because the non-meaningful tokens are collapsed into a `DerivedToken`, not deleted.

This step makes it simpler to process the patterns going forward, as we do not have to care about whether an abbreviated month had a period after it or not, or if there were any extra spaces.

We will start over on ex1:

[source, ruby]
----
ex1 = Emendate.process('[Jan. 1st - February 9, 2013]')
 => #<Emendate::ProcessingManager:1540
  @state=done,
  token_type_pattern: [:range_date_type]>

ex1.orig_tokens.types
 => [:square_bracket_open,
 :month_abbr_alpha,
 :single_dot,
 :space,
 :number1or2,
 :ordinal_indicator,
 :space,
 :hyphen,
 :space,
 :month_alpha,
 :space,
 :number1or2,
 :comma,
 :space,
 :number4,
 :square_bracket_close]

ex1.collapsed_tokens.types
=> [:square_bracket_open,
 :month_abbr_alpha,
 :number1or2,
 :ordinal_indicator,
 :hyphen,
 :month_alpha,
 :number1or2,
 :comma,
 :number4,
 :square_bracket_close]
----

Here, the `:single_dot` and `:space` tokens have been collapsed into the `:month_abbr_alpha` token. The `:space` tokens surrounding the `:hyphen` and following the `:comma` have also been collapsed.

If we look at the `:month_abbr_alpha` token, we can see that it contains multiple level of source tokens,footnote:[This is because the `:space` following the `:single_dot` is first collapsed into a `DerivedToken` with type `:single_dot.` Then the derived `:single_dot` is collapsed into `:month_abbr_alpha`. I https://github.com/kspurgin/emendate/issues/8[plan to eventually simplify the hierarchy of source tokens that is created].] but that none of the original tokens have been thrown away:

[source, ruby]
----
ex1.collapsed_tokens[1]
 => #<Emendate::DerivedToken:0x00007fa41fa9a400
 @certainty=[],
 @lexeme="jan.",
 @literal=nil,
 @location=#<struct Location col=1, length=5>, <1>
 @sources=
  #<Emendate::SegmentSets::MixedSet:0x00007fa41fa9aae0
   @certainty=[],
   @inferred_date=false,
   @segments=
    [#<Emendate::Token:0x00007fa41faa21f0 @certainty=[], @lexeme="jan", @literal=nil, @location=#<struct Location col=1, length=3>, @type=:month_abbr_alpha>,
     #<Emendate::DerivedToken:0x00007fa41fa9b260
      @certainty=[],
      @lexeme=".",
      @literal=nil,
      @location=#<struct Location col=4, length=2>,
      @sources=
       #<Emendate::SegmentSets::MixedSet:0x00007fa41fa9b1e8
        @certainty=[],
        @inferred_date=false,
        @segments=
         [#<Emendate::Token:0x00007fa41faa21c8 @certainty=[], @lexeme=".", @literal=nil, @location=#<struct Location col=4, length=1>, @type=:single_dot>,
          #<Emendate::Token:0x00007fa41faa21a0 @certainty=[], @lexeme=" ", @literal=nil, @location=#<struct Location col=5, length=1>, @type=:space>],
        @warnings=[]>,
      @type=:single_dot>],
   @warnings=[]>,
 @type=:month_abbr_alpha>
----
<1> The location length of the final derived token is `5`, because it represents "jan. ".

=== `converted_months`

*Produced by processing step: `:convert_months`*

Takes `collapsed_tokens` and converts any ``Token``s with type `:month_alpha` or `:month_abbr_alpha` to ``DatePart``s with type `:month`.

This step is just another simplification, so that we don't have to treat "January" and "Jan." separately.

Note that this does not handle identification/conversion of `:number1or2` tokens that are months.

----
ex1.converted_months.types
 => [:square_bracket_open,
 :month,
 :number1or2,
 :ordinal_indicator,
 :hyphen,
 :month,
 :number1or2,
 :comma,
 :number4,
 :square_bracket_close]
----

=== `translated_ordinals`

*Produced by processing step: `:translate_ordinals`*

Removes ordinal indicators appearing as expected after numbers.

This step is just another simplification, so that we don't have to treat "1" and "1st" separately.

----
ex1.translated_ordinals.types
 => [:square_bracket_open,
 :month,
 :number1or2,
 :hyphen,
 :month,
 :number1or2,
 :comma,
 :number4,
 :square_bracket_close]
----

=== `certainty_checked`

*Produced by processing step: `:certainty_check`*

Encodes the following `certainty` attributes on the `SegmentSet` as appropriate when it applies to the entire date string:

- `:supplied` (when whole string enclosed in [] and EDTF options are not applied)
- `:approximate` (when preceded by circa or if ~ or % is at end of date string)
- `:uncertain` (when ? or % is at end of date string)
- `:one_of_set` (when whole string enclosed in [] and EDTF options are applied)
- `:all_of_set` (when whole string enclosed in {})

[source, ruby]
----
ex1.certainty_checked.types
 => [:month, :number1or2, :hyphen, :month, :number1or2, :comma, :number4]
ex1.certainty_checked.certainty
 => [:supplied]
----

The default interpretation of square brackets is as supplied/inferred date, so that value is set. We no longer have to deal with the brackets as separate tokens.

If you working with EDTF, this step will encode EDTF group and individual element certainty values to relevant ``Segment``s as appropriate:

- `:approximate` (applies to individual segment)
- `:uncertain` (applies to individual segment)
- `:leftward_approximate` (applies to individual segment and all previous segments in the same date -- the final handling of this must happen after date segmenting)
- `:leftward_uncertain` (applies to individual segment and all previous segments in the same date -- the final handling of this must happen after date segmenting)

[source, ruby]
----
ex2 = Emendate.process('~2004-06-%11')
ex2.certainty_checked.types
 => [:number4, :hyphen, :number1or2, :hyphen, :number1or2]
ex2.certainty_checked[0].type
 => :number4
ex2.certainty_checked[0].certainty
 => [:approximate]
ex2.certainty_checked[4].type
 => :number1or2
ex2.certainty_checked[4].certainty
 => [:approximate, :uncertain]
----

For full documentation, run the following from the base `emendate` directory:

`rspec spec/lib/emendate/certainty_checker_spec.rb`

=== `standardized_formats`

*Produced by processing step: `:standardize_formats`*

Carries out many manipulations on the `SegmentSet` to standardize it in preparation for ``Token``s to be tagged as ``DatePart``s.

For full documentation, run the following from the base `emendate` directory:

`rspec spec/lib/emendate/format_standardizer_spec.rb`

.Fills in missing date elements (inserts year for Jan 1) and removes comma after "February 9"
[source, ruby]
----
ex1.standardized_formats.types
=> [:month, :number1or2, :number4, :hyphen, :month, :number1or2, :number4]
----

=== `tagged_date_parts`

*Produced by processing step: `:tag_date_parts`*

Turns remaining eligible ``Token``s into ``DatePart``s.

In our running example, the `:number1or2` tokens are converted to `:day` ``DatePart``s:

[source, ruby]
----
ex1.tagged_date_parts.types
 => [:month, :day, :year, :hyphen, :month, :day, :year]
----

In the following example, ``Token``s with types `:number1or2` (18) and `:century` (cent.) are collapsed into one `DatePart` with type `:century` and literal value `18`.

----
ex4 = Emendate.process('early 18th cent.')
ex4.standardized_formats.types
=> [:partial, :number1or2, :century]
ex4.tagged_date_parts.types
=> [:partial, :century]
ex4.tagged_date_parts[1].class
=> Emendate::DatePart
ex4.tagged_date_parts[1].literal
=> 18
ex4.tagged_date_parts[1].lexeme
=> "18cent"
----

=== `segmented_dates`

*Produced by processing step: `:segment_dates`*

Collapses the ``DatePart``s that make up a given date into one `DateType`.

----
ex1.segmented_dates.types
=> [:yearmonthday_date_type, :hyphen, :yearmonthday_date_type]
----

Also includes `:partial`, `:before`, and `:after` tokens in the construction of the `DateType`.

----
ex4.segmented_dates.types
=> [:century_date_type]
ex4.segmented_dates[0].partial_indicator
=> "early"
ex4.segmented_dates[0].earliest
=> #<Date: 1701-01-01 ((2342338j,0s,0n),+0s,2299161j)>
ex4.segmented_dates[0].latest
=> #<Date: 1734-12-31 ((2354755j,0s,0n),+0s,2299161j)>
----

=== `ranges_indicated`

*Produced by processing step: `:indicate_ranges`*

Collapses ``DateType``s separated by a `range_indicator` into a single `Range` `DateType`.

[source, ruby]
----
ex1.ranges_indicated.types
 => [:range_date_type]
ex1.orig_string
 => "[Jan. 1st - February 9, 2013]"
ex1.ranges_indicated[0].earliest
 => Tue, 01 Jan 2013
ex1.ranges_indicated[0].latest
 => Sat, 09 Feb 2013
----
